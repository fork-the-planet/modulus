# SPDX-FileCopyrightText: Copyright (c) 2023 - 2026 NVIDIA CORPORATION & AFFILIATES.
# SPDX-FileCopyrightText: All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""Distributed AFNO layer implementations.

This module provides distributed versions of AFNO building blocks for
model-parallel training across multiple GPUs.
"""

from __future__ import annotations

import math
import warnings
from typing import Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from jaxtyping import Float
from torch import Tensor

import physicsnemo
from physicsnemo.distributed.manager import DistributedManager
from physicsnemo.distributed.mappings import (
    copy_to_parallel_region,
    gather_from_parallel_region,
    reduce_from_parallel_region,
    scatter_to_parallel_region,
)
from physicsnemo.distributed.utils import compute_split_shapes


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases
    # Method based on
    # https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        low = norm_cdf((a - mean) / std)
        up = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [low, up], then translate to
        # [2low-1, 2up-1].
        tensor.uniform_(2 * low - 1, 2 * up - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def _trunc_normal_(
    tensor: Tensor,
    mean: float = 0.0,
    std: float = 1.0,
    a: float = -2.0,
    b: float = 2.0,
) -> Tensor:
    r"""Fill the input tensor with values from a truncated normal distribution.

    The values are drawn from :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within the bounds.
    The method works best when :math:`a \leq \text{mean} \leq b`.

    Parameters
    ----------
    tensor : torch.Tensor
        An n-dimensional tensor to fill.
    mean : float, optional, default=0.0
        Mean of the normal distribution.
    std : float, optional, default=1.0
        Standard deviation of the normal distribution.
    a : float, optional, default=-2.0
        Minimum cutoff value.
    b : float, optional, default=2.0
        Maximum cutoff value.

    Returns
    -------
    torch.Tensor
        The input tensor filled with truncated normal values.

    Examples
    --------
    >>> w = torch.empty(3, 5)
    >>> o = _trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


@torch.compile
def drop_path(
    x: Float[Tensor, "*dims"], drop_prob: float = 0.0, training: bool = False
) -> Float[Tensor, "*dims"]:
    r"""Drop paths (Stochastic Depth) per sample.

    When applied in the main path of residual blocks, this implements
    stochastic depth regularization. Note that this is different from
    DropConnect despite some naming confusion in literature.

    See `TPU discussion <https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956>`_
    for more details. Opted for changing the layer and argument names to 'drop path' rather
    than mix DropConnect as a layer name and use 'survival rate' as the argument.

    Parameters
    ----------
    x : torch.Tensor
        Input tensor of any shape.
    drop_prob : float, optional, default=0.0
        Probability of dropping a path.
    training : bool, optional, default=False
        Whether model is in training mode.

    Returns
    -------
    torch.Tensor
        Output tensor with same shape as input.
    """
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1.0 - drop_prob
    # Work with different dimensional tensors, not just 2D ConvNets
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(physicsnemo.Module):
    r"""Drop paths (Stochastic Depth) per sample.

    When applied in the main path of residual blocks, this implements
    stochastic depth regularization.

    Parameters
    ----------
    drop_prob : float, optional
        Probability of dropping a path during training.

    Forward
    -------
    x : torch.Tensor
        Input tensor of any shape.

    Outputs
    -------
    torch.Tensor
        Output tensor with same shape as input.

    Examples
    --------
    >>> import torch
    >>> from physicsnemo.models.afno.distributed.layers import DropPath
    >>> layer = DropPath(drop_prob=0.1)
    >>> x = torch.randn(2, 64, 8, 8)
    >>> out = layer(x)
    >>> out.shape
    torch.Size([2, 64, 8, 8])
    """

    def __init__(self, drop_prob: float = None):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x: Tensor) -> Tensor:
        r"""Forward pass applying stochastic depth."""
        return drop_path(x, self.drop_prob, self.training)


class DistributedMLP(physicsnemo.Module):
    r"""Distributed MLP layer for model-parallel training.

    This MLP distributes the hidden layer computation across the model parallel
    group for memory efficiency.

    Parameters
    ----------
    in_features : int
        Size of input features.
    hidden_features : int, optional
        Size of hidden features. Defaults to ``in_features``.
    out_features : int, optional
        Size of output features. Defaults to ``in_features``.
    act_layer : type, optional, default=nn.GELU
        Activation layer class.
    drop : float, optional, default=0.0
        Dropout rate.
    input_is_matmul_parallel : bool, optional, default=False
        Whether input is already sharded across model parallel group.
    output_is_matmul_parallel : bool, optional, default=False
        Whether output should be sharded across model parallel group.

    Forward
    -------
    x : torch.Tensor
        Input tensor of shape :math:`(B, C, H, W)`.

    Outputs
    -------
    torch.Tensor
        Output tensor of shape :math:`(B, C_{out}, H, W)`.

    Examples
    --------
    Requires a distributed environment with model parallel group initialized.

    >>> import torch  # doctest: +SKIP
    >>> from physicsnemo.models.afno.distributed.layers import DistributedMLP  # doctest: +SKIP
    >>> from physicsnemo.distributed.manager import DistributedManager  # doctest: +SKIP
    >>> DistributedManager.initialize()  # doctest: +SKIP
    >>> mlp = DistributedMLP(in_features=256, hidden_features=1024, out_features=256)  # doctest: +SKIP
    >>> x = torch.randn(2, 256, 4, 4)  # doctest: +SKIP
    >>> out = mlp(x)  # doctest: +SKIP
    >>> out.shape  # doctest: +SKIP
    torch.Size([2, 256, 4, 4])
    """

    def __init__(
        self,
        in_features: int,
        hidden_features: int | None = None,
        out_features: int | None = None,
        act_layer: type = nn.GELU,
        drop: float = 0.0,
        input_is_matmul_parallel: bool = False,
        output_is_matmul_parallel: bool = False,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.input_is_matmul_parallel = input_is_matmul_parallel
        self.output_is_matmul_parallel = output_is_matmul_parallel

        # get effective embedding size:
        comm_size = DistributedManager().group_size("model_parallel")
        if not (hidden_features % comm_size == 0):
            raise ValueError(
                "Error, hidden_features needs to be divisible by matmul_parallel_size"
            )
        hidden_features_local = hidden_features // comm_size

        # first set of hp
        self.w1 = nn.Parameter(torch.ones(hidden_features_local, in_features, 1, 1))
        self.b1 = nn.Parameter(torch.zeros(hidden_features_local))

        # second set of hp
        self.w2 = nn.Parameter(torch.ones(out_features, hidden_features_local, 1, 1))
        self.b2 = nn.Parameter(torch.zeros(out_features))

        self.act = act_layer()
        self.drop = nn.Dropout(drop) if drop > 0.0 else nn.Identity()

        if self.input_is_matmul_parallel:
            self.gather_shapes = compute_split_shapes(
                in_features, DistributedManager().group_size("model_parallel")
            )

        # init weights
        self._init_weights()

    def _init_weights(self) -> None:
        r"""Initialize weights using truncated normal distribution."""
        _trunc_normal_(self.w1, std=0.02)
        nn.init.constant_(self.b1, 0.0)
        _trunc_normal_(self.w2, std=0.02)
        nn.init.constant_(self.b2, 0.0)

    def forward(self, x: Float[Tensor, "B C H W"]) -> Float[Tensor, "B C_out H W"]:
        r"""Forward pass of the distributed MLP."""
        # Gather if input is model parallel
        if self.input_is_matmul_parallel:
            x = gather_from_parallel_region(
                x, dim=1, shapes=self.gather_shapes, group="model_parallel"
            )

        # Distribute computation across model parallel group
        x = copy_to_parallel_region(x, group="model_parallel")
        x = F.conv2d(x, self.w1, bias=self.b1)
        x = self.act(x)
        x = self.drop(x)
        x = F.conv2d(x, self.w2, bias=None)
        x = reduce_from_parallel_region(x, group="model_parallel")
        x = x + torch.reshape(self.b2, (1, -1, 1, 1))
        x = self.drop(x)

        # Scatter if output should be model parallel
        if self.output_is_matmul_parallel:
            x = scatter_to_parallel_region(x, dim=1, group="model_parallel")

        return x


class DistributedPatchEmbed(physicsnemo.Module):
    r"""Distributed patch embedding layer for model-parallel training.

    Converts 2D patches into a 1D vector sequence, distributed across
    the model parallel group.

    Parameters
    ----------
    inp_shape : Tuple[int, int], optional, default=(224, 224)
        Input image dimensions as ``(height, width)``.
    patch_size : Tuple[int, int], optional, default=(16, 16)
        Patch size as ``(patch_height, patch_width)``.
    in_chans : int, optional, default=3
        Number of input channels.
    embed_dim : int, optional, default=768
        Embedding dimension.
    input_is_matmul_parallel : bool, optional, default=False
        Whether input is already sharded across model parallel group.
    output_is_matmul_parallel : bool, optional, default=True
        Whether output should be sharded across model parallel group.

    Forward
    -------
    x : torch.Tensor
        Input tensor of shape :math:`(B, C_{in}, H, W)`.

    Outputs
    -------
    torch.Tensor
        Output tensor of shape :math:`(B, C_{embed}, N)` where :math:`N` is
        the number of patches.

    Examples
    --------
    Requires a distributed environment with model parallel group initialized.

    >>> import torch  # doctest: +SKIP
    >>> from physicsnemo.models.afno.distributed.layers import DistributedPatchEmbed  # doctest: +SKIP
    >>> from physicsnemo.distributed.manager import DistributedManager  # doctest: +SKIP
    >>> DistributedManager.initialize()  # doctest: +SKIP
    >>> embed = DistributedPatchEmbed(  # doctest: +SKIP
    ...     inp_shape=(64, 64), in_chans=2, embed_dim=256,
    ...     output_is_matmul_parallel=False,
    ... )
    >>> x = torch.randn(2, 2, 64, 64)  # doctest: +SKIP
    >>> out = embed(x)  # doctest: +SKIP
    >>> out.shape  # doctest: +SKIP
    torch.Size([2, 256, 16])  # 16 = num_patches
    """

    def __init__(
        self,
        inp_shape: Tuple[int, int] = (224, 224),
        patch_size: Tuple[int, int] = (16, 16),
        in_chans: int = 3,
        embed_dim: int = 768,
        input_is_matmul_parallel: bool = False,
        output_is_matmul_parallel: bool = True,
    ):
        super().__init__()

        # store params
        self.input_parallel = input_is_matmul_parallel
        self.output_parallel = output_is_matmul_parallel

        # get comm sizes:
        matmul_comm_size = DistributedManager().group_size("model_parallel")

        # compute parameters
        num_patches = (inp_shape[1] // patch_size[1]) * (inp_shape[0] // patch_size[0])
        self.inp_shape = (inp_shape[0], inp_shape[1])
        self.patch_size = patch_size
        self.num_patches = num_patches

        if self.input_parallel:
            if not (in_chans % matmul_comm_size == 0):
                raise ValueError(
                    "Error, the in_chans needs to be divisible by matmul_parallel_size"
                )
            self.in_shapes = compute_split_shapes(
                in_chans, DistributedManager().group_size("model_parallel")
            )

        # get effective embedding size:
        if self.output_parallel:
            if not (embed_dim % matmul_comm_size == 0):
                raise ValueError(
                    "Error, the embed_dim needs to be divisible by matmul_parallel_size"
                )
            out_chans_local = embed_dim // matmul_comm_size
        else:
            out_chans_local = embed_dim

        # the weights  of this layer is shared across spatial parallel ranks
        self.proj = nn.Conv2d(
            in_chans, out_chans_local, kernel_size=patch_size, stride=patch_size
        )

        # make sure we reduce them across rank
        self.proj.weight.is_shared_spatial = True
        self.proj.bias.is_shared_spatial = True

    def forward(self, x: Float[Tensor, "B C_in H W"]) -> Float[Tensor, "B C N"]:
        r"""Forward pass of the distributed patch embedding."""
        # Gather input if model parallel
        if self.input_parallel:
            x = gather_from_parallel_region(
                x, dim=1, shapes=self.in_shapes, group="model_parallel"
            )

        # Copy to parallel region if output should be distributed
        if self.output_parallel:
            x = copy_to_parallel_region(x, group="model_parallel")

        # Input validation (single check: shape must match expected)
        if not torch.compiler.is_compiling():
            expected_spatial = (self.inp_shape[0], self.inp_shape[1])
            expected_chans = self.proj.in_channels
            if (
                x.ndim != 4
                or x.shape[1] != expected_chans
                or (x.shape[2], x.shape[3]) != expected_spatial
            ):
                raise ValueError(
                    f"Expected input shape (B, {expected_chans}, {expected_spatial[0]}, "
                    f"{expected_spatial[1]}), got {tuple(x.shape)}"
                )

        # Apply patch embedding: (B, C_in, H, W) -> (B, C_embed, num_patches)
        x = self.proj(x).flatten(2)
        return x


@torch.compile
def _compl_mul_add_fwd(
    a: torch.Tensor, b: torch.Tensor, c: torch.Tensor
) -> torch.Tensor:
    r"""Complex multiplication and addition using real representation.

    Computes ``einsum(a, b) + c`` where tensors represent complex numbers
    as real-valued tensors with a trailing dimension of size 2.

    Parameters
    ----------
    a : torch.Tensor
        First input tensor with complex values in real representation.
    b : torch.Tensor
        Second input tensor (weights) with complex values in real representation.
    c : torch.Tensor
        Bias tensor with complex values in real representation.

    Returns
    -------
    torch.Tensor
        Result tensor with complex values in real representation.
    """
    tmp = torch.einsum("bkixys,kiot->stbkoxy", a, b)
    res = (
        torch.stack(
            [tmp[0, 0, ...] - tmp[1, 1, ...], tmp[1, 0, ...] + tmp[0, 1, ...]], dim=-1
        )
        + c
    )
    return res


@torch.compile
def _compl_mul_add_fwd_c(
    a: torch.Tensor, b: torch.Tensor, c: torch.Tensor
) -> torch.Tensor:
    r"""Complex multiplication and addition using native complex tensors.

    Computes ``einsum(a, b) + c`` using PyTorch's native complex tensor support.

    Parameters
    ----------
    a : torch.Tensor
        First input tensor with complex values in real representation.
    b : torch.Tensor
        Second input tensor (weights) with complex values in real representation.
    c : torch.Tensor
        Bias tensor with complex values in real representation.

    Returns
    -------
    torch.Tensor
        Result tensor with complex values in real representation.
    """
    ac = torch.view_as_complex(a)
    bc = torch.view_as_complex(b)
    cc = torch.view_as_complex(c)
    tmp = torch.einsum("bkixy,kio->bkoxy", ac, bc)
    res = tmp + cc
    return torch.view_as_real(res)


class DistributedAFNO2D(physicsnemo.Module):
    r"""Distributed AFNO 2D spectral convolution layer.

    This layer performs spectral mixing using block-diagonal weight matrices
    in the Fourier domain, distributed across the model parallel group.

    Parameters
    ----------
    hidden_size : int
        Feature dimensionality.
    num_blocks : int, optional, default=8
        Number of blocks in the block diagonal weight matrix.
    sparsity_threshold : float, optional, default=0.01
        Sparsity threshold for soft shrinkage.
    hard_thresholding_fraction : float, optional, default=1
        Fraction of modes to keep, in range ``[0, 1]``.
    hidden_size_factor : int, optional, default=1
        Factor to increase spectral features by after weight multiplication.
    input_is_matmul_parallel : bool, optional, default=False
        Whether input is already sharded across model parallel group.
    output_is_matmul_parallel : bool, optional, default=False
        Whether output should be sharded across model parallel group.

    Forward
    -------
    x : torch.Tensor
        Input tensor of shape :math:`(B, C, H, W)`.

    Outputs
    -------
    torch.Tensor
        Output tensor of shape :math:`(B, C, H, W)`.

    Examples
    --------
    Requires a distributed environment with model parallel group initialized.

    >>> import torch  # doctest: +SKIP
    >>> from physicsnemo.models.afno.distributed.layers import DistributedAFNO2D  # doctest: +SKIP
    >>> from physicsnemo.distributed.manager import DistributedManager  # doctest: +SKIP
    >>> DistributedManager.initialize()  # doctest: +SKIP
    >>> layer = DistributedAFNO2D(hidden_size=256, num_blocks=8)  # doctest: +SKIP
    >>> x = torch.randn(2, 256, 4, 4)  # doctest: +SKIP
    >>> out = layer(x)  # doctest: +SKIP
    >>> out.shape  # doctest: +SKIP
    torch.Size([2, 256, 4, 4])
    """

    def __init__(
        self,
        hidden_size: int,
        num_blocks: int = 8,
        sparsity_threshold: float = 0.01,
        hard_thresholding_fraction: float = 1,
        hidden_size_factor: int = 1,
        input_is_matmul_parallel: bool = False,
        output_is_matmul_parallel: bool = False,
    ):
        super().__init__()
        if not (hidden_size % num_blocks == 0):
            raise ValueError(
                f"hidden_size {hidden_size} should be divisible by num_blocks {num_blocks}"
            )

        # get comm sizes:
        matmul_comm_size = DistributedManager().group_size("model_parallel")

        self.fft_handle = torch.fft.rfft2
        self.ifft_handle = torch.fft.irfft2

        self.hidden_size = hidden_size
        self.sparsity_threshold = sparsity_threshold
        self.num_blocks = num_blocks
        if not (self.num_blocks % matmul_comm_size == 0):
            raise ValueError(
                "Error, num_blocks needs to be divisible by matmul_parallel_size"
            )
        self.num_blocks_local = self.num_blocks // matmul_comm_size
        self.block_size = self.hidden_size // self.num_blocks
        self.hard_thresholding_fraction = hard_thresholding_fraction
        self.hidden_size_factor = hidden_size_factor
        self.scale = 0.02
        use_complex_mult = False
        self._mult_handle = (
            _compl_mul_add_fwd_c if use_complex_mult else _compl_mul_add_fwd
        )

        # model parallelism
        self.input_is_matmul_parallel = input_is_matmul_parallel
        self.output_is_matmul_parallel = output_is_matmul_parallel

        # new
        # these weights need to be synced across all spatial ranks!
        self.w1 = nn.Parameter(
            self.scale
            * torch.randn(
                self.num_blocks_local,
                self.block_size,
                self.block_size * self.hidden_size_factor,
                2,
            )
        )
        self.b1 = nn.Parameter(
            self.scale
            * torch.randn(
                self.num_blocks_local,
                self.block_size * self.hidden_size_factor,
                1,
                1,
                2,
            )
        )
        self.w2 = nn.Parameter(
            self.scale
            * torch.randn(
                self.num_blocks_local,
                self.block_size * self.hidden_size_factor,
                self.block_size,
                2,
            )
        )
        self.b2 = nn.Parameter(
            self.scale * torch.randn(self.num_blocks_local, self.block_size, 1, 1, 2)
        )

        # make sure we reduce them across rank
        self.w1.is_shared_spatial = True
        self.b1.is_shared_spatial = True
        self.w2.is_shared_spatial = True
        self.b2.is_shared_spatial = True

    def forward(self, x: Float[Tensor, "B C H W"]) -> Float[Tensor, "B C H W"]:
        r"""Forward pass of the distributed AFNO spectral layer."""
        # Scatter input across model parallel group if needed
        if not self.input_is_matmul_parallel:
            num_chans = x.shape[1]
            x = scatter_to_parallel_region(x, dim=1, group="model_parallel")

        # Store for skip connection
        bias = x

        dtype = x.dtype
        x = x.float()
        B, C, H, W = x.shape
        total_modes = H // 2 + 1
        kept_modes = int(total_modes * self.hard_thresholding_fraction)

        # Apply 2D FFT to spatial dimensions
        x = self.fft_handle(x, (H, W), (-2, -1), "ortho")
        x = x.view(B, self.num_blocks_local, self.block_size, H, W // 2 + 1)

        # Convert to real representation for block-diagonal operations
        x = torch.view_as_real(x)
        o2 = torch.zeros(x.shape, device=x.device)

        # Apply first block-diagonal weight matrix with ReLU
        o1 = F.relu(
            self._mult_handle(
                x[
                    :,
                    :,
                    :,
                    total_modes - kept_modes : total_modes + kept_modes,
                    :kept_modes,
                    :,
                ],
                self.w1,
                self.b1,
            )
        )

        # Apply second block-diagonal weight matrix
        o2[
            :, :, :, total_modes - kept_modes : total_modes + kept_modes, :kept_modes, :
        ] = self._mult_handle(o1, self.w2, self.b2)

        # Apply soft shrinkage for sparsity and inverse FFT
        x = F.softshrink(o2, lambd=self.sparsity_threshold)
        x = torch.view_as_complex(x)
        x = x.reshape(B, C, H, W // 2 + 1)
        x = self.ifft_handle(x, (H, W), (-2, -1), "ortho")
        x = x.type(dtype) + bias

        # Gather output if not model parallel
        if not self.output_is_matmul_parallel:
            gather_shapes = compute_split_shapes(
                num_chans, DistributedManager().group_size("model_parallel")
            )
            x = gather_from_parallel_region(
                x, dim=1, shapes=gather_shapes, group="model_parallel"
            )

        return x
